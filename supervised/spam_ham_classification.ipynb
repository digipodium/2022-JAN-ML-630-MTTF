{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cjRgw6ClHgp"
      },
      "source": [
        "# NLP process \n",
        "- Text analysis  50%\n",
        "- Text Transformation 40%\n",
        "- Model development 5%\n",
        "- ui application 5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlFnM7sJlHgq"
      },
      "source": [
        "# text analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "IkMwfsuilHgv",
        "outputId": "11f4be06-6235-4734-e96f-823eef9272fa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(plt.style.available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6GtOSTlHg0"
      },
      "outputs": [],
      "source": [
        "# plt.style.available    --- use to see available themes\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bPWeMflHg5"
      },
      "outputs": [],
      "source": [
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "pip install nltk\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAocaKZLlHg9"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4gwgAZrOmq6R",
        "outputId": "9853f470-affd-4ac4-a251-a7d32d83358a"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLFHhKHKlHhF"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2349</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3624</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4685</th>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2030</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1518</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4807</th>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     category                                               text  label_num\n",
              "605       ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
              "2349      ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
              "3624      ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
              "4685     spam  Subject: photoshop , windows , office . cheap ...          1\n",
              "2030      ham  Subject: re : indian springs\\r\\nthis deal is t...          0\n",
              "...       ...                                                ...        ...\n",
              "1518      ham  Subject: put the 10 on the ft\\r\\nthe transport...          0\n",
              "404       ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...          0\n",
              "2933      ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...          0\n",
              "1409      ham  Subject: industrial worksheets for august 2000...          0\n",
              "4807     spam  Subject: important online banking alert\\r\\ndea...          1\n",
              "\n",
              "[5171 rows x 3 columns]"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = pd.read_csv('../datasets/spam_ham_dataset.csv',index_col=0)\n",
        "messages.rename({'label':'category'},axis=1,inplace=True)\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# text tranformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('','',string.punctuation))\n",
        "    words = word_tokenize(text)\n",
        "    # print(\" \".join(words),len(words))\n",
        "    words = [w for w in words if w not in stopwords.words('english')]\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    words = [stemmer.stem(w) for w in words]\n",
        "    # print(\" \".join(words),len(words))\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages.text = messages.text.apply(cleanText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer(max_features=1000)\n",
        "X = cv.fit_transform(messages.text).toarray()\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tf = TfidfTransformer()\n",
        "X = tf.fit_transform(messages.text).toarray()\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [1, 5171]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Admin\\Documents\\2022JANML630MTTF\\supervised\\spam_ham_classification.ipynb Cell 22'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Documents/2022JANML630MTTF/supervised/spam_ham_classification.ipynb#ch0000070?line=0'>1</a>\u001b[0m xtrain,xtest,ytrain,ytest \u001b[39m=\u001b[39m train_test_split(X,messages\u001b[39m.\u001b[39;49mlabel_num,test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,random_state\u001b[39m=\u001b[39;49m\u001b[39m123\u001b[39;49m)\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\env2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2413'>2414</a>\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2414'>2415</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2416'>2417</a>\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2418'>2419</a>\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2419'>2420</a>\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2420'>2421</a>\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/model_selection/_split.py?line=2421'>2422</a>\u001b[0m )\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\env2\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=358'>359</a>\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=359'>360</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=360'>361</a>\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=373'>374</a>\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=374'>375</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=376'>377</a>\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=377'>378</a>\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=378'>379</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32m~\\miniconda3\\envs\\env2\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=329'>330</a>\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=330'>331</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=331'>332</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=332'>333</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=333'>334</a>\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    <a href='file:///c%3A/Users/Admin/miniconda3/envs/env2/lib/site-packages/sklearn/utils/validation.py?line=334'>335</a>\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 5171]"
          ]
        }
      ],
      "source": [
        "xtrain,xtest,ytrain,ytest = train_test_split(X,messages.label_num,test_size=0.2,random_state=123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = GaussianNB()\n",
        "clf.fit(xtrain,ytrain)\n",
        "ypred = clf.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(confusion_matrix(ytest,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(classification_report(ytest,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "email = messages.text[4685]\n",
        "clean_email = cleanText(email)\n",
        "X = cv.transform([clean_email]).toarray()\n",
        "p = clf.predict(X)\n",
        "if p[0] == 1:\n",
        "    print(\"Spam\")\n",
        "else:\n",
        "    print(\"Ham\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "spam_ham_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "25d61629bec000cc03289752b02dcba135edb76a863a22362471406107f610cf"
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 64-bit ('base': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
